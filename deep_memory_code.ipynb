{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the OpenAI key\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "activeloop_key = os.getenv(\"ACTIVELOOP_TOKEN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAIChat\n",
    "from langchain.vectorstores.deeplake import DeepLake\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "import random\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://siddartha10/manufacturing_CSI already exists, loading from the storage\n"
     ]
    }
   ],
   "source": [
    "# dataset_path = \"hub://101010/text_embedding\"\n",
    "# # Create a DeepLake instance and add the documents\n",
    "# db = DeepLake.from_documents(documents, dataset_path=dataset_path, embedding=OpenAIEmbeddings())\n",
    "\n",
    "db = DeepLake(\n",
    "    dataset_path=f\"hub://siddartha10/manufacturing_CSI\",  # org_id stands for your username or organization from activeloop\n",
    "    embedding=openai_embeddings,\n",
    "    runtime={\"tensor_db\": True},\n",
    "    token=activeloop_key,\n",
    "    # overwrite=True, # user overwrite flag if you want to overwrite the full dataset\n",
    "    read_only=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'file_paths' with the paths to your local PDF files\n",
    "file_paths = [\n",
    "    \"manual\\'s.pdf\",\n",
    "    \"Operation and Maintenance Manual.pdf\",\n",
    "    \"Operations Manual.pdf\"\n",
    "]\n",
    "# Initialize an empty list to store all pages\n",
    "pages = []\n",
    "\n",
    "# Loop through each manual and load its pages\n",
    "for manual_path in file_paths:\n",
    "    loader = PyPDFLoader(manual_path)\n",
    "    pages1 = loader.load_and_split()\n",
    "    pages.extend(pages1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size = 4096\n",
    "docs_new = []\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    ")\n",
    "\n",
    "for doc in pages:\n",
    "    if len(doc.page_content) < chunk_size:\n",
    "        docs_new.append(doc)\n",
    "    else:\n",
    "        docs = text_splitter.create_documents([doc.page_content])\n",
    "        docs_new.extend(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 197 embeddings in 1 batches of size 197:: 100%|██████████| 1/1 [00:11<00:00, 11.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://siddartha10/manufacturing_CSI', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype       shape      dtype  compression\n",
      "  -------    -------     -------    -------  ------- \n",
      " embedding  embedding  (591, 1536)  float32   None   \n",
      "    id        text      (591, 1)      str     None   \n",
      " metadata     json      (591, 1)      str     None   \n",
      "   text       text      (591, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "docs = db.add_documents(docs_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain.chains.openai_functions import (\n",
    "    create_structured_output_chain,\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset docs and ids if they exist (optional you can also ingest)\n",
    "docs = db.vectorstore.dataset.text.data(fetch_chunks=True, aslist=True)[\"value\"]\n",
    "ids = db.vectorstore.dataset.id.data(fetch_chunks=True, aslist=True)[\"value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "591\n"
     ]
    }
   ],
   "source": [
    "print(len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['18789c18-930f-11ee-b6ec-7ae137960975', '18789d76-930f-11ee-b6ec-7ae137960975', '18789d94-930f-11ee-b6ec-7ae137960975', '18789da8-930f-11ee-b6ec-7ae137960975', '18789dc6-930f-11ee-b6ec-7ae137960975']\n"
     ]
    }
   ],
   "source": [
    "print(ids[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a world class expert for generating questions based on provided context.                 You make sure the question can be answered by the text.\n",
      "Human: Use the given text to generate a question from the following input: \n",
      "UWM CSI Vial Filling Connected Smart Manufacturing (CSM) System The UWM CSI Vial Filling CSM  System is an intelligent manufacturing system using the latest Industry 4.0 connected advanced manufacturing equipment and techniques to produce  vials filled with varying product using a variety of filling methods and capturing process data that can be used for data analysis and system optimization.  The  Vial Filling CSM System is a platform that university faculty and students will use for both education and research to  further the advancement of a connected enterprise.   The CAM components are  integrate d seamlessly within a fully  integrated architecture and connected  enterprise using cutting -edge smart -data devices at all layers.     The process overview detailed in this section describes in general how the Vial Filling Connected Smart Manufacturing system functions as a complete system processing components.  Operational steps and slight variances in the process may differ from what is described here depending on the configuration parameters or  using the stations  in a dry cycle mode .  The Vial Filling CSM utilizes localized system configuration setting within the machine as well as process data requirements and parameters to determine how  to process the product and which stations and inspections are required to complete the production job.  When running under MES Production Center control, the process requirements and parameters are received  from the MES system as to where  and how to process and inspect the vials produced by the system .    \n",
      "Human: Tips: Make sure to answer in the correct format\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "question='What is the purpose of the UWM CSI Vial Filling Connected Smart Manufacturing (CSM) System?'\n"
     ]
    }
   ],
   "source": [
    "# If we pass in a model explicitly, we need to make sure it supports the OpenAI function-calling API.\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "class Questions(BaseModel):\n",
    "    \"\"\"Identifying information about the manufacturing system.\"\"\"\n",
    "\n",
    "    question: str = Field(..., description=\"Questions about manufacturing system\")\n",
    "\n",
    "\n",
    "prompt_msgs = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a world class expert for generating questions based on provided context. \\\n",
    "                You make sure the question can be answered by the text.\"\n",
    "    ),\n",
    "    HumanMessagePromptTemplate.from_template(\n",
    "        \"Use the given text to generate a question from the following input: {input}\"\n",
    "    ),\n",
    "    HumanMessage(content=\"Tips: Make sure to answer in the correct format\"),\n",
    "]\n",
    "prompt = ChatPromptTemplate(messages=prompt_msgs)\n",
    "chain = create_structured_output_chain(Questions, llm, prompt, verbose=True)\n",
    "\n",
    "text = \"\"\"\n",
    "UWM CSI Vial Filling Connected Smart Manufacturing (CSM) System The UWM CSI Vial Filling CSM  System is an intelligent manufacturing system using the latest Industry 4.0 connected advanced manufacturing equipment and techniques to produce  vials filled with varying product using a variety of filling methods and capturing process data that can be used for data analysis and system optimization.  The  Vial Filling CSM System is a platform that university faculty and students will use for both education and research to  further the advancement of a connected enterprise.   The CAM components are  integrate d seamlessly within a fully  integrated architecture and connected  enterprise using cutting -edge smart -data devices at all layers.     The process overview detailed in this section describes in general how the Vial Filling Connected Smart Manufacturing system functions as a complete system processing components.  Operational steps and slight variances in the process may differ from what is described here depending on the configuration parameters or  using the stations  in a dry cycle mode .  The Vial Filling CSM utilizes localized system configuration setting within the machine as well as process data requirements and parameters to determine how  to process the product and which stations and inspections are required to complete the production job.  When running under MES Production Center control, the process requirements and parameters are received  from the MES system as to where  and how to process and inspect the vials produced by the system .    \"\"\"\n",
    "questions = chain.run(input=text)\n",
    "print(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q: 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q: 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q: 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q: 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q: 110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q: 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q: 130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q: 140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q: 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q: 160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q: 170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q: 180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q: 190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [10:19<00:00,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_queries(docs: List[str], ids: List[str], n: int = 100):\n",
    "    questions = []\n",
    "    relevances = []\n",
    "    pbar = tqdm(total=n)\n",
    "    while len(questions) < n:\n",
    "        # 1. randomly draw a piece of text and relevance id\n",
    "        r = random.randint(0, len(docs) - 1)\n",
    "        text, label = docs[r], ids[r]\n",
    "\n",
    "        # 2. generate queries and assign and relevance id\n",
    "        generated_qs = [chain.run(input=text).question]\n",
    "        questions.extend(generated_qs)\n",
    "        relevances.extend([[(label, 1)] for _ in generated_qs])\n",
    "        pbar.update(len(generated_qs))\n",
    "        if len(questions) % 10 == 0:\n",
    "            print(f\"q: {len(questions)}\")\n",
    "    return questions[:n], relevances[:n]\n",
    "\n",
    "\n",
    "chain = create_structured_output_chain(Questions, llm, prompt, verbose=False)\n",
    "questions, relevances = generate_queries(docs, ids, n=200)\n",
    "\n",
    "train_questions, train_relevances = questions[:100], relevances[:100]\n",
    "test_questions, test_relevances = questions[100:], relevances[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100\n"
     ]
    }
   ],
   "source": [
    "print(len(train_questions), len(train_relevances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_relevances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting DeepMemory training job\n",
      "Your Deep Lake dataset has been successfully created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training data for deepmemory:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 100 embeddings in 1 batches of size 100:: 100%|██████████| 1/1 [00:05<00:00,  5.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepMemory training job started. Job ID: 656f62265ea8c423468c338c\n"
     ]
    }
   ],
   "source": [
    "job_id = db.vectorstore.deep_memory.train(\n",
    "    queries=train_questions,\n",
    "    relevance=train_relevances,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/siddartha10/manufacturing_CSI\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a real number, not 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/siddartha/Desktop/github/ActiveloopQA/deep_memory_code.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/siddartha/Desktop/github/ActiveloopQA/deep_memory_code.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m status \u001b[39m=\u001b[39m db\u001b[39m.\u001b[39;49mvectorstore\u001b[39m.\u001b[39;49mdeep_memory\u001b[39m.\u001b[39;49mstatus(\u001b[39m\"\u001b[39;49m\u001b[39m656f62265ea8c423468c338c\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/siddartha/Desktop/github/ActiveloopQA/deep_memory_code.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(status[\u001b[39m\"\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/deeplake/core/vectorstore/deep_memory/deep_memory.py:53\u001b[0m, in \u001b[0;36maccess_control.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m     \u001b[39mraise\u001b[39;00m DeepMemoryWaitingListError()\n\u001b[0;32m---> 53\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/deeplake/core/vectorstore/deep_memory/deep_memory.py:313\u001b[0m, in \u001b[0;36mDeepMemory.status\u001b[0;34m(self, job_id)\u001b[0m\n\u001b[1;32m    311\u001b[0m     recall \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    312\u001b[0m     improvement \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 313\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mcheck_status(job_id\u001b[39m=\u001b[39;49mjob_id, recall\u001b[39m=\u001b[39;49mrecall, improvement\u001b[39m=\u001b[39;49mimprovement)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/deeplake/client/client.py:609\u001b[0m, in \u001b[0;36mDeepMemoryBackendClient.check_status\u001b[0;34m(self, job_id, recall, improvement)\u001b[0m\n\u001b[1;32m    607\u001b[0m check_response_status(response)\n\u001b[1;32m    608\u001b[0m response_status_schema \u001b[39m=\u001b[39m JobResponseStatusSchema(response\u001b[39m=\u001b[39mresponse\u001b[39m.\u001b[39mjson())\n\u001b[0;32m--> 609\u001b[0m response_status_schema\u001b[39m.\u001b[39;49mprint_status(job_id, recall, improvement)\n\u001b[1;32m    610\u001b[0m \u001b[39mreturn\u001b[39;00m response\u001b[39m.\u001b[39mjson()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/deeplake/client/utils.py:159\u001b[0m, in \u001b[0;36mJobResponseStatusSchema.print_status\u001b[0;34m(self, job_id, recall, improvement)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[39mif\u001b[39;00m response[\u001b[39m\"\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcompleted\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 159\u001b[0m     response[\u001b[39m\"\u001b[39m\u001b[39mresults\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m get_results(\n\u001b[1;32m    160\u001b[0m         response\u001b[39m=\u001b[39;49mresponse,\n\u001b[1;32m    161\u001b[0m         indent\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m*\u001b[39;49m \u001b[39m30\u001b[39;49m,\n\u001b[1;32m    162\u001b[0m         add_vertical_bars\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    163\u001b[0m         recall\u001b[39m=\u001b[39;49mrecall,\n\u001b[1;32m    164\u001b[0m         improvement\u001b[39m=\u001b[39;49mimprovement,\n\u001b[1;32m    165\u001b[0m     )\n\u001b[1;32m    167\u001b[0m \u001b[39mprint\u001b[39m(line)\n\u001b[1;32m    168\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m|\u001b[39m\u001b[39m{:^60}\u001b[39;00m\u001b[39m|\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(response[\u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m]))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/deeplake/client/utils.py:284\u001b[0m, in \u001b[0;36mget_results\u001b[0;34m(response, improvement, recall, indent, add_vertical_bars, width)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[39mfor\u001b[39;00m progress_key, progress_value \u001b[39min\u001b[39;00m progress\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    282\u001b[0m     \u001b[39mif\u001b[39;00m progress_key \u001b[39m==\u001b[39m BEST_RECALL:\n\u001b[1;32m    283\u001b[0m         \u001b[39m# verify that the recall and improvement coincide with the best recall\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m         recall, improvement \u001b[39m=\u001b[39m get_best_recall_improvement(\n\u001b[1;32m    285\u001b[0m             recall, improvement, progress_value\n\u001b[1;32m    286\u001b[0m         )\n\u001b[1;32m    287\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m improvement:\n\u001b[1;32m    288\u001b[0m             improvement \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(+\u001b[39m\u001b[39m{\u001b[39;00mimprovement\u001b[39m}\u001b[39;00m\u001b[39m%)\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/deeplake/client/utils.py:296\u001b[0m, in \u001b[0;36mget_best_recall_improvement\u001b[0;34m(recall, improvement, best_recall)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_best_recall_improvement\u001b[39m(recall, improvement, best_recall):\n\u001b[1;32m    295\u001b[0m     brecall, bimprovement \u001b[39m=\u001b[39m get_recall_improvement(best_recall)\n\u001b[0;32m--> 296\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mfloat\u001b[39;49m(improvement) \u001b[39m>\u001b[39m \u001b[39mfloat\u001b[39m(bimprovement):\n\u001b[1;32m    297\u001b[0m         \u001b[39mreturn\u001b[39;00m recall, improvement\n\u001b[1;32m    298\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mfloat\u001b[39m(improvement) \u001b[39m<\u001b[39m \u001b[39mfloat\u001b[39m(bimprovement):\n",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a real number, not 'NoneType'"
     ]
    }
   ],
   "source": [
    "status = db.vectorstore.deep_memory.status(\"656f62265ea8c423468c338c\")\n",
    "print(status[\"status\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding queries took 2.00 seconds\n",
      "---- Evaluating without Deep Memory ---- \n",
      "Recall@1:\t  0.0%\n",
      "Recall@3:\t  0.0%\n",
      "Recall@5:\t  0.0%\n",
      "Recall@10:\t  0.0%\n",
      "Recall@50:\t  0.0%\n",
      "Recall@100:\t  0.0%\n",
      "---- Evaluating with Deep Memory ---- \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Deepmemory model not found in the dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/siddartha/Desktop/github/ActiveloopQA/deep_memory_code.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/siddartha/Desktop/github/ActiveloopQA/deep_memory_code.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m recall \u001b[39m=\u001b[39m db\u001b[39m.\u001b[39;49mvectorstore\u001b[39m.\u001b[39;49mdeep_memory\u001b[39m.\u001b[39;49mevaluate(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/siddartha/Desktop/github/ActiveloopQA/deep_memory_code.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     queries\u001b[39m=\u001b[39;49mtest_questions,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/siddartha/Desktop/github/ActiveloopQA/deep_memory_code.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     relevance\u001b[39m=\u001b[39;49mtest_relevances,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/siddartha/Desktop/github/ActiveloopQA/deep_memory_code.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/deeplake/core/vectorstore/deep_memory/deep_memory.py:53\u001b[0m, in \u001b[0;36maccess_control.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m     \u001b[39mraise\u001b[39;00m DeepMemoryWaitingListError()\n\u001b[0;32m---> 53\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/deeplake/core/vectorstore/deep_memory/deep_memory.py:515\u001b[0m, in \u001b[0;36mDeepMemory.evaluate\u001b[0;34m(self, relevance, queries, embedding_function, embedding, top_k, qvs_params)\u001b[0m\n\u001b[1;32m    513\u001b[0m eval_type \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mwith\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m use_model \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mwithout\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    514\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m---- Evaluating \u001b[39m\u001b[39m{\u001b[39;00meval_type\u001b[39m}\u001b[39;00m\u001b[39m Deep Memory ---- \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 515\u001b[0m avg_recalls, queries_dict \u001b[39m=\u001b[39m recall_at_k(\n\u001b[1;32m    516\u001b[0m     indra_dataset,\n\u001b[1;32m    517\u001b[0m     relevance,\n\u001b[1;32m    518\u001b[0m     top_k\u001b[39m=\u001b[39;49mtop_k,\n\u001b[1;32m    519\u001b[0m     query_embs\u001b[39m=\u001b[39;49mquery_embs,\n\u001b[1;32m    520\u001b[0m     metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[1;32m    521\u001b[0m     use_model\u001b[39m=\u001b[39;49muse_model,\n\u001b[1;32m    522\u001b[0m )\n\u001b[1;32m    524\u001b[0m queries_data\u001b[39m.\u001b[39mupdate(queries_dict)\n\u001b[1;32m    526\u001b[0m \u001b[39mfor\u001b[39;00m recall, recall_value \u001b[39min\u001b[39;00m avg_recalls\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/deeplake/core/vectorstore/deep_memory/deep_memory.py:601\u001b[0m, in \u001b[0;36mrecall_at_k\u001b[0;34m(indra_dataset, relevance, query_embs, metric, top_k, use_model)\u001b[0m\n\u001b[1;32m    598\u001b[0m correct_labels \u001b[39m=\u001b[39m [rel[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m rel \u001b[39min\u001b[39;00m query_relevance]\n\u001b[1;32m    600\u001b[0m \u001b[39m# Compute the cosine similarity between the query and all data points\u001b[39;00m\n\u001b[0;32m--> 601\u001b[0m view \u001b[39m=\u001b[39m get_view(\n\u001b[1;32m    602\u001b[0m     metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[1;32m    603\u001b[0m     query_emb\u001b[39m=\u001b[39;49mquery_emb,\n\u001b[1;32m    604\u001b[0m     indra_dataset\u001b[39m=\u001b[39;49mindra_dataset,\n\u001b[1;32m    605\u001b[0m )\n\u001b[1;32m    607\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m top_k:\n\u001b[1;32m    608\u001b[0m     collect_data \u001b[39m=\u001b[39m k \u001b[39m==\u001b[39m \u001b[39m10\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/deeplake/core/vectorstore/deep_memory/deep_memory.py:651\u001b[0m, in \u001b[0;36mget_view\u001b[0;34m(metric, query_emb, indra_dataset, return_tensors, tql_filter)\u001b[0m\n\u001b[1;32m    649\u001b[0m return_tensors_str \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(return_tensors)\n\u001b[1;32m    650\u001b[0m tql \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSELECT * FROM (SELECT \u001b[39m\u001b[39m{\u001b[39;00mreturn_tensors_str\u001b[39m}\u001b[39;00m\u001b[39m, ROW_NUMBER() as indices, \u001b[39m\u001b[39m{\u001b[39;00mmetric\u001b[39m}\u001b[39;00m\u001b[39m(embedding, ARRAY[\u001b[39m\u001b[39m{\u001b[39;00mquery_emb_str\u001b[39m}\u001b[39;00m\u001b[39m]) as score \u001b[39m\u001b[39m{\u001b[39;00mtql_filter_str\u001b[39m}\u001b[39;00m\u001b[39m order by \u001b[39m\u001b[39m{\u001b[39;00mmetric\u001b[39m}\u001b[39;00m\u001b[39m(embedding, ARRAY[\u001b[39m\u001b[39m{\u001b[39;00mquery_emb_str\u001b[39m}\u001b[39;00m\u001b[39m]) desc limit 100)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 651\u001b[0m indra_view \u001b[39m=\u001b[39m indra_dataset\u001b[39m.\u001b[39;49mquery(tql)\n\u001b[1;32m    652\u001b[0m \u001b[39mreturn\u001b[39;00m indra_view\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Deepmemory model not found in the dataset"
     ]
    }
   ],
   "source": [
    "recall = db.vectorstore.deep_memory.evaluate(\n",
    "    queries=test_questions,\n",
    "    relevance=test_relevances,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
